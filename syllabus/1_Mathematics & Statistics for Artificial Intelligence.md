# ACTS, Pune

## Suggested Teaching Guidelines for  
**Mathematics & Statistics for Artificial Intelligence  
PG-DAI February 2025**

### Duration: 40 Classroom hours  

**Objective:** To reinforce knowledge of general Aptitude, Mathematics, and Statistics concepts  

**Prerequisites:** Knowledge of basic Mathematics.  

### Evaluation Method:  
- Theory exam: 80% weightage  
- Internal exam: 20% weightage  

### List of Books / Other Training Material  

#### Reference Books:  
1. *An Introduction to Statistical Learning: with Applications in R* by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie. ISBN 9781461471387  
2. *Advanced Engineering Mathematics* by Erwin Kreyszig. ISBN 978-8126531356  
3. *Linear Algebra* by Jim Hefferon. ISBN 978-1944325039  
4. *Higher Engineering Mathematics* by B V Ramana. ISBN 978-0070634190  
5. *The Elements of Statistical Learning: Data Mining, Inference & Prediction* by Trevor Hastie, Jerome Friedman  

#### Notes:  
- Each session is 2 hours long for theory.  
- Faculty is advised to relate mathematical concepts to real-world applications.  
- Faculty is advised to connect topics with machine learning algorithms.  

---

## Linear Algebra  

### Session 1 & 2  
**Lecture:**  
- Vectors: Definition  
- Scalars, Addition, Scalar Multiplication  
- Inner Product (Dot Product), Vector Projection  
- Cosine Similarity, Orthogonal Vectors  

### Session 3 & 4  
**Lecture:**  
- Normal and Orthonormal Vectors  
- Vector Norm, Vector Space  
- Linear Combination, Linear Span  
- Linear Independence, Basis Vectors  

**Assignment:**  
Consider the vectors {[3, 0, 4], [-1, 0, 7], [2, 9, 11]}. Check if the vectors are linearly independent.  

### Session 5  
**Lecture:**  
- Linear Independence  
- Basis and Rank  
- Linear Mappings  
- Affine Spaces  

### Session 6 & 7  
**Lecture:**  
- Matrices: Definition, Addition, Transpose  
- Scalar Multiplication, Matrix Multiplication, Matrix Multiplication Properties  
- Hadamard Product, Functions, Linear Transformation, Determinant, Identity Matrix  
- Invertible  

**Assignment:**  
Given \( X = [0 \ 1 \ 3]^T \) and \( Y = [2 \ 4 \ 0]^T \):  
- Find \( V = \) Subspace of \( X \)  
- Find \( W = \) Subspace of \( Y \)  
- Describe \( V \cap W \)  

### Session 8 & 9  
**Lecture:**  
- Matrix and Inverse, Rank, Trace  
- Popular Types of Matrices: Symmetric, Diagonal, Orthogonal, Orthonormal  
- Positive Definite Matrix  
- Matrix Phylogeny  
- Matrix Approximation  

### Session 10 & 11  
**Lecture:**  
- Eigenvalues & Eigenvectors: Concept, Intuition, Significance  
- How to Find Principle Component Analysis  
- Concept, Properties, Applications  
- Singular Value Decomposition  

**Assignment:**  
Find the eigenvalues and eigenvectors for the matrix:  
\[
\begin{bmatrix}
5 & -10 & -5 \\
2 & 14 & 2 \\
-4 & -8 & 6
\end{bmatrix}
\]

---

## Calculus  

### Session 12 & 13  
**Lecture:**  
- Functions, Scalar Derivative: Definition, Intuition  
- Common Rules of Differentiation, Chain Rule  

### Session 14 & 15  
**Lecture:**  
- Partial Derivatives, Gradient  
- Concept, Intuition, Properties  
- Directional Derivative  

**Assignment:**  
Let \( f(x,y,z) = xy e^{x^2} + z^2 - 5 \).  
1. Calculate the gradient of \( f \) at the point \( (1,3,-2) \).  
2. Calculate the directional derivative \( D_u f \) at \( (1,3,-2) \) in the direction of \( v = (3,-1,4) \).  

### Session 16 & 17  
**Lecture:**  
- Gradients of Vector-Valued Functions  
- Gradient of Matrices  
- Useful Identities for Computing Gradients  
- Backpropagation and Automatic Differentiation  
- Linearization and Multivariate Taylor Series  

### Session 18  
**Lecture:**  
- Optimization Using Gradient Descent  
- Constrained Optimization and Lagrange Multipliers  
- Convex Optimization  

### Session 19  
**Lecture:**  
- Vector and Matrix Calculus  
- How to Find Derivative of Scalar-Valued and Vector-Valued Functions  
- Four Combinations-Jacobian  

### Session 20  
**Lecture:**  
- Gradient Algorithms, Local/Global Maxima and Minima  
- Saddle Point, Convex Functions  
- Gradient Descent Algorithms: Batch, Mini-Batch, Stochastic  
- Performance Comparison  