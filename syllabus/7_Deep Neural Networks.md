# ACTS, Pune  
## Suggested Teaching Guidelines for  
**Deep Neural Networks PG-DAI February 2025**  

### Duration:  
- **40 hours Theory**  
- **50 hours Lab**  

### Objective:  
Deep Neural Network concepts and applications.  

### Prerequisites:  
Knowledge of programming fundamentals and basic mathematics & statistics.  

### Evaluation Method:  
- Theory exam: 40%  
- Lab exam: 40%  
- Internal exam: 20%  

---

## List of Books / Other Training Material  

### Courseware:  
*Deep Learning using Python*, S. Lovelyn Rose, L. Ashok Kumar, D. Karthika Renuka, Wiley India  

### Reference Books:  
1. *Deep Learning with Python* by Francis Chollet  
2. *Deep Learning* by Ian Goodfellow, Yoshua Bengio, Aaron Courville  
3. *Neural Networks and Learning Machines* by Simon Haykin  
4. *Pattern Recognition and Machine Learning* by Christopher M. Bishop  
5. *Hands-On Machine Learning with Scikit-Learn and TensorFlow*  
6. *TensorFlow Deep Learning Cookbook*  
7. *Reinforcement Learning with TensorFlow* by Sayon Dutta  
8. *Hands-On Reinforcement Learning with Python* by Sudharsan Ravichandiran  
9. *Deep Reinforcement Learning Hands-On* by Maxim Lapan  

---

## Course Outline  

### Session 1  
**Lecture:**  
- Introduction to Deep Neural Networks  
- Single-layer Neural Networks  
- Activation Functions (Sigmoid, Tanh, ReLU)  
- Backpropagation Overview  

**Lab Assignment:**  
- Implement activation functions in Jupyter Notebook.  

---

### Session 2  
**Lecture:**  
- Introduction to TensorFlow and PyTorch  
- Comparison of TensorFlow vs. PyTorch  

**Lab Assignment:**  
- Explore TensorFlow and PyTorch libraries.  

---

### Session 3 & 4  
**Lecture:**  
- "Hello World" of Neural Networks (Logistic Regression)  
- Cost Function, Gradient Descent, Backpropagation  

**Lab Assignment:**  
- Implement forward/backward propagation.  
- Build and optimize a shallow neural network.  

---

### Session 5 (2T + 4L)  
**Lecture:**  
- Sigmoid Model, Loss Function  
- Gradient Descent Update Rule  

**Lab Assignment:**  
- Plot Sigmoid functions (2D/3D), loss curves.  
- Standardize data and split into train/test sets.  

---

### Session 6 (2T + 4L)  
**Lecture:**  
- Shallow Neural Networks  
- Hidden Layers, Tanh/ReLU Activations  

**Lab Assignment:**  
- Implement a two-class neural network with cross-entropy loss.  

---

### Session 7 & 8  
**Lecture:**  
- Hyperparameters vs. Parameters  
- Regularization (L1/L2, Frobenius Norm)  

**Lab Assignment:**  
- Apply L1/L2 regularization to improve model performance.  

---

### Session 9  
**Lecture:**  
- Dropout, Early Stopping, Data Augmentation  

**Lab Assignment:**  
- Implement dropout and data augmentation techniques.  

---

### Session 10  
**Lecture:**  
- Vanishing/Exploding Gradients  
- Gradient Checking  

**Lab Assignment:**  
- Debug gradient issues in a neural network.  

---

### Session 11 (2T + 4L)  
**Lecture:**  
- Batch Normalization  

**Lab Assignment:**  
- Implement normalization methods.  

---

### Session 12  
**Lecture:**  
- Optimization Algorithms (Adam, Mini-batch GD)  

**Lab Assignment:**  
- Implement Adam optimizer in TensorFlow.  

---

### Session 13  
**Lecture:**  
- RMSProp, Momentum  

**Lab Assignment:**  
- Compare gradient descent variants.  

---

### Session 14 & 15  
**Lecture:**  
- TensorFlow Data Structures  

**Lab Assignment:**  
- Work with Fashion MNIST and Digits MNIST datasets.  

---

### Session 16 (2T + 4L)  
**Lecture:**  
- Sequential Models, RNNs  

**Lab Assignment:**  
- Implement RNNs in Jupyter Notebook.  

---

### Session 17 & 18  
**Lecture:**  
- CNNs: Convolution, Pooling, Transfer Learning  

**Lab Assignment:**  
- Build a CNN for American Sign Language (ASL) recognition.  

---

### Session 19 (2T + 4L)  
**Lecture:**  
- GANs: Introduction and Implementation  

**Lab Assignment:**  
- Implement a GAN using TensorFlow.  

---

### Session 20  
**Lecture:**  
- Model Tuning (Layers, Neurons)  
- Trends: Autoencoders, Extreme Learning  

**Lab Assignment:**  
- Implement CNN/RNN for case studies (e.g., Iris Detection).  

---

### Self-Study:  
Reinforcement Learning (MDPs, Monte Carlo, TD Learning).  

--- 
